{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Problem statement\n",
    "Implementation of a face detection neural network based on Transformer.\n",
    "\n",
    "## What is Transformer\n",
    "Self-attention-based architectures, in particular Transformers, have become the model of choice in natural language processing (NLP). The dominant approach is to pre-train on a large text corpus and then fine-tune on a smaller task-specific dataset. Thanks to Transformersâ€™ computational efficiency and scalability, it has become possible to train models of unprecedented size, with over 100B parameters. With the models and datasets growing, there is still no sign of saturating performance.\n",
    "\n",
    "## Transformer in Computer Vision\n",
    "Take vision transformer (ViT) as example. There are two principles for implementing this structure. One is to implement the code in pytorch directly by splitting the input image into multiple patches, each patch is directly straightened as a vector, an image is split into as many patches, and then these vectors are used as input to the transformer, and then the whole learning process uses only the encoder structure to expand. The final output links a multilayer perceptron, which is a fully connected layer, for classification. The other is to segment the input image into multiple patches, and each patch is fed into a CNN that extracts a 1D tensor as the word vector of this patch. The latter process is the same."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Challenges and solution\n",
    "In Computer Vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention, some replacing the convolutions entirely. The latter models, while theoretically efficient, have not yet been scaled effectively on modern hardware accelerators due to the use of specialized attention patterns. \n",
    "\n",
    "On the other hand, applications like human face detection usually use a pre-trained CNN architecture with attention. Since the system is more likely to be deployed on mobile platforms, the aforementioned CNN architectures incorporating attention is difficult to be deployed reasonably on such platforms with low computing efficiency.\n",
    "\n",
    "**Solution**: Inspired by the Transformer scaling successes in NLP, instead of modifying the architecture of CNN, ViT suggests applying a standard Transformer directly to images, with the fewest possible modifications. Specifically, an image is splited into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. \n",
    "In this case, attention is incorporated at the cost of little computations.\n",
    "\n",
    "\n",
    "However, in our early trials, the ViT model fails to achieve high performance on test data. We found the performance increases as the parameter, `patch_size` continues to increase, from 16, to 56. Hereby we give a possible reason for this. Our input image is $(224, 224)$ shape tensor, and according to ViT, the image_size must be divisible by `patch_size`. So we initially set it to `patch_size=16`. In this case, the input image is splited into $(224//16)^2=196$ patches. Considering that the whole area of an entire image is a human face and there is no additional background, it is clear that a patch as large as 196 divides the face region too finely, resulting in the model not learning the identity information. So, we finally found a proper value of `patch_size`, which is 56.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Human face detection using ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Human face images are used in this presentation. We download the whole dataset from\n",
    "sklearn, which contains 1288 gray image samples of 7 labels with each sample of a\n",
    "shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1288, 2914), (1288, 62, 47))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=64)\n",
    "# (1288, 2914), (1288, 62, 47)\n",
    "faces.data.shape, faces.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "However, somehow all the images need to be pixel flipped before they can be displayed properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(255-faces.images[0, :, :], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Dataset\n",
    "All the images above is divided into training set and testing set using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "images = 255-faces.images\n",
    "labels = faces.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, shuffle=True, stratify=labels)\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    ]\n",
    ")\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, item):\n",
    "        sample = self.transform(self.x[item]) if self.transform is not None else self.x[item]\n",
    "        label = self.y[item]\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1030, 62, 47), (258, 62, 47), (1030,), (258,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "model selection\n",
    "Original ViT is used in this demo.\n",
    "To preserve the position space information of the patches in the image, position embedding information similar to that used in NLP is added before the word vector input, so there is no difference in the use of position information by using only 1D position information (similar to using only 1D sequential numbering 1, 2, 3... for each patch position in the image) compared to using 2D (similar to 2D encoding representation of (x,y) in the inclusion space). ...), there is no difference compared to using 2D (similar to the 2D encoded representation of (x,y) containing space). (Similar application feels similar to the 19 years JD fine-grained DCL, selfconcept, location information in addition to the relationship between the information of different locations in the same image, the learning process takes more into account the same part of the same kind of different images of the same location area, such as a part of him under all images of this class of things)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from vit_pytorch import ViT\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "model = ViT(\n",
    "    image_size = 224,\n",
    "    patch_size = 56,\n",
    "    num_classes = 7,\n",
    "    dim = 16,\n",
    "    heads = 3,\n",
    "    mlp_dim = 164,\n",
    "    depth = 4,\n",
    "    channels = 1\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "lr = 0.001\n",
    "num_epoch = 100\n",
    "validate_every = 4\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=FaceDataset(x_train, y_train, transforms), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=FaceDataset(x_test, y_test, transforms), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def draw_loss(loss_train, loss_test):\n",
    "    plt.title(\"loss\")\n",
    "    x = np.arange(0, len(loss_train))\n",
    "    plt.plot(x, np.array(loss_train), label='train')\n",
    "    x = np.arange(0, len(loss_train), validate_every)\n",
    "    plt.plot(x, np.array(loss_test), label='test')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 1.2012, acc:0.5909: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:01<00:00, 28.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 0 | val_loss : 0.8196 | val_acc: 0.6840\n",
      "Current best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.3981, acc:0.8690: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 43.13it/s]\n",
      "epoch: 2, loss: 0.1666, acc:0.9564: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 45.02it/s]\n",
      "epoch: 3, loss: 0.0638, acc:0.9915: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 46.54it/s]\n",
      "epoch: 4, loss: 0.0180, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 48.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 4 | val_loss : 0.2243 | val_acc: 0.9340\n",
      "Current best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 5, loss: 0.0065, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 49.17it/s]\n",
      "epoch: 6, loss: 0.0042, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 48.89it/s]\n",
      "epoch: 7, loss: 0.0027, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 44.95it/s]\n",
      "epoch: 8, loss: 0.0022, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 43.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 8 | val_loss : 0.2613 | val_acc: 0.8819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 9, loss: 0.0017, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 43.82it/s]\n",
      "epoch: 10, loss: 0.0016, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 43.59it/s]\n",
      "epoch: 11, loss: 0.0013, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 42.96it/s]\n",
      "epoch: 12, loss: 0.0011, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 42.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 12 | val_loss : 0.2262 | val_acc: 0.9375\n",
      "Current best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 13, loss: 0.0010, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 42.00it/s]\n",
      "epoch: 14, loss: 0.0008, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 40.86it/s]\n",
      "epoch: 15, loss: 0.0008, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.85it/s]\n",
      "epoch: 16, loss: 0.0007, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 40.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 16 | val_loss : 0.2107 | val_acc: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 17, loss: 0.0006, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.19it/s]\n",
      "epoch: 18, loss: 0.0006, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.61it/s]\n",
      "epoch: 19, loss: 0.0005, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 42.18it/s]\n",
      "epoch: 20, loss: 0.0005, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 42.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 20 | val_loss : 0.2015 | val_acc: 0.9340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 21, loss: 0.0004, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.82it/s]\n",
      "epoch: 22, loss: 0.0004, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 42.28it/s]\n",
      "epoch: 23, loss: 0.0004, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 40.79it/s]\n",
      "epoch: 24, loss: 0.0004, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 24 | val_loss : 0.2131 | val_acc: 0.9444\n",
      "Current best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 25, loss: 0.0003, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.95it/s]\n",
      "epoch: 26, loss: 0.0003, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.51it/s]\n",
      "epoch: 27, loss: 0.0003, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.51it/s]\n",
      "epoch: 28, loss: 0.0003, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 40.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 28 | val_loss : 0.2078 | val_acc: 0.9479\n",
      "Current best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 29, loss: 0.0002, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.30it/s]\n",
      "epoch: 30, loss: 0.0002, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.50it/s]\n",
      "epoch: 31, loss: 0.0002, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.26it/s]\n",
      "epoch: 32, loss: 0.0002, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 32 | val_loss : 0.2024 | val_acc: 0.9410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 33, loss: 0.0002, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.45it/s]\n",
      "epoch: 34, loss: 0.0002, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.93it/s]\n",
      "epoch: 35, loss: 0.0002, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.98it/s]\n",
      "epoch: 36, loss: 0.0002, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 36 | val_loss : 0.2126 | val_acc: 0.9410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 37, loss: 0.0002, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.82it/s]\n",
      "epoch: 38, loss: 0.0002, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 40.88it/s]\n",
      "epoch: 39, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.61it/s]\n",
      "epoch: 40, loss: 0.0002, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 42.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 40 | val_loss : 0.2211 | val_acc: 0.9444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 41, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 39.99it/s]\n",
      "epoch: 42, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 40.22it/s]\n",
      "epoch: 43, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.98it/s]\n",
      "epoch: 44, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 44 | val_loss : 0.3441 | val_acc: 0.8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 45, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 42.41it/s]\n",
      "epoch: 46, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 40.64it/s]\n",
      "epoch: 47, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.45it/s]\n",
      "epoch: 48, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 40.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 48 | val_loss : 0.2058 | val_acc: 0.9444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 49, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.87it/s]\n",
      "epoch: 50, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 40.84it/s]\n",
      "epoch: 51, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.32it/s]\n",
      "epoch: 52, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 52 | val_loss : 0.2197 | val_acc: 0.9340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 53, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 40.19it/s]\n",
      "epoch: 54, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 40.29it/s]\n",
      "epoch: 55, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.40it/s]\n",
      "epoch: 56, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 42.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 56 | val_loss : 0.2143 | val_acc: 0.9444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 57, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 42.03it/s]\n",
      "epoch: 58, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.51it/s]\n",
      "epoch: 59, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 42.25it/s]\n",
      "epoch: 60, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 60 | val_loss : 0.2071 | val_acc: 0.9410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 61, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.69it/s]\n",
      "epoch: 62, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.40it/s]\n",
      "epoch: 63, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.74it/s]\n",
      "epoch: 64, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 42.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 64 | val_loss : 0.4175 | val_acc: 0.8924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 65, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.87it/s]\n",
      "epoch: 66, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.29it/s]\n",
      "epoch: 67, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 39.94it/s]\n",
      "epoch: 68, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 68 | val_loss : 0.2137 | val_acc: 0.9444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 69, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.35it/s]\n",
      "epoch: 70, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.61it/s]\n",
      "epoch: 71, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 40.88it/s]\n",
      "epoch: 72, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 42.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 72 | val_loss : 0.2068 | val_acc: 0.9410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 73, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 42.91it/s]\n",
      "epoch: 74, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 43.02it/s]\n",
      "epoch: 75, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.93it/s]\n",
      "epoch: 76, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 42.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 76 | val_loss : 0.2273 | val_acc: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 77, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.50it/s]\n",
      "epoch: 78, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 42.41it/s]\n",
      "epoch: 79, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 42.35it/s]\n",
      "epoch: 80, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 40.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 80 | val_loss : 0.2243 | val_acc: 0.9410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 81, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 40.41it/s]\n",
      "epoch: 82, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 39.85it/s]\n",
      "epoch: 83, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 40.49it/s]\n",
      "epoch: 84, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 40.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 84 | val_loss : 0.2299 | val_acc: 0.9410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 85, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 40.39it/s]\n",
      "epoch: 86, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 40.84it/s]\n",
      "epoch: 87, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.38it/s]\n",
      "epoch: 88, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 88 | val_loss : 0.2095 | val_acc: 0.9410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 89, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.61it/s]\n",
      "epoch: 90, loss: 0.0000, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.04it/s]\n",
      "epoch: 91, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.45it/s]\n",
      "epoch: 92, loss: 0.0001, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 40.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 92 | val_loss : 0.2221 | val_acc: 0.9410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 93, loss: 0.0047, acc:0.9949: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 39.61it/s]\n",
      "epoch: 94, loss: 1.6288, acc:0.6600: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.41it/s]\n",
      "epoch: 95, loss: 0.5030, acc:0.8438: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.13it/s]\n",
      "epoch: 96, loss: 0.1387, acc:0.9527: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Validating==========================\n",
      "Epoch : 96 | val_loss : 0.4110 | val_acc: 0.8611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 97, loss: 0.0336, acc:0.9962: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 39.94it/s]\n",
      "epoch: 98, loss: 0.0104, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 40.93it/s]\n",
      "epoch: 99, loss: 0.0049, acc:1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 41.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "loss_train_all = []\n",
    "loss_test_all = []\n",
    "best_acc = 0\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    loss_train_epoch = []\n",
    "    loss_test_epoch = []\n",
    "    loop = tqdm(train_loader)\n",
    "    for data, label in loop:\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = (output.argmax(dim=1) == label).float().mean()\n",
    "        epoch_accuracy += acc / len(train_loader)\n",
    "        epoch_loss += loss.cpu().detach().numpy() / len(train_loader)\n",
    "        loop.set_description(f\"epoch: {epoch}, loss: {epoch_loss:.4f}, acc:{epoch_accuracy:.4f}\")\n",
    "        loss_train_epoch.append(epoch_loss)\n",
    "    loss_train_all.append(np.array(loss_train_epoch).mean())\n",
    "    if 0 == epoch%validate_every:\n",
    "        print('==========================Validating==========================')\n",
    "        with torch.no_grad():\n",
    "            epoch_val_accuracy = 0\n",
    "            epoch_val_loss = 0\n",
    "            for data, label in test_loader:\n",
    "                data = data.to(device)\n",
    "                label = label.to(device)\n",
    "                val_output = model(data)\n",
    "                val_loss = criterion(val_output, label)\n",
    "                acc = (val_output.argmax(dim=1) == label).float().mean()\n",
    "                epoch_val_accuracy += acc / len(test_loader)\n",
    "                epoch_val_loss += val_loss.cpu().detach().numpy() / len(test_loader)\n",
    "                loss_test_epoch.append(epoch_val_loss)\n",
    "            loss_test_all.append(np.array(loss_test_epoch).mean())\n",
    "            print(f\"Epoch : {epoch} | val_loss : {epoch_val_loss:.4f} | val_acc: {epoch_val_accuracy:.4f}\")\n",
    "        if epoch_val_accuracy > best_acc:\n",
    "            best_acc = epoch_val_accuracy\n",
    "            print('Current best model')\n",
    "            torch.save(model.state_dict(), f\"./weights/model-{epoch}-{epoch_val_accuracy:.4f}.pth\")\n",
    "print('==================================================================')\n",
    "draw_loss(loss_train_all, loss_test_all)\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluation\n",
    "The best accuracy on test data is over 94%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
